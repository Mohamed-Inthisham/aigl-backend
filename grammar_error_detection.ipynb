{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\anaconda3\\envs\\elarning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\moham\\anaconda3\\envs\\elarning\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, os, torch\n",
    "from transformers import (\n",
    "                        T5Tokenizer,\n",
    "                        T5ForConditionalGeneration,\n",
    "                        TrainingArguments, \n",
    "                        Trainer\n",
    "                        )\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOTS = [\n",
    "        'data/fce/json/fce.train.json',\n",
    "        'data/fce/json/fce.dev.json',\n",
    "        'data/fce/json/fce.test.json'\n",
    "        ]\n",
    "\n",
    "SPLITS = [\n",
    "        'train', \n",
    "        'valid', \n",
    "        'test'\n",
    "        ]\n",
    "\n",
    "save_dir = 'data/fce/final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_multiple_substrings(\n",
    "                                original_string, \n",
    "                                replacements\n",
    "                                ):\n",
    "    replacements.sort(key=lambda x: x[0])\n",
    "    result = original_string\n",
    "    offset = 0  \n",
    "    for start_index, end_index, new_substring in replacements:\n",
    "        adjusted_start = start_index + offset\n",
    "        adjusted_end = end_index + offset\n",
    "        if adjusted_start < 0 or adjusted_end > len(result) or adjusted_start > adjusted_end:\n",
    "            print(f\"Error: Invalid indices for replacement '{new_substring}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        result = result[:adjusted_start] + str(new_substring) + result[adjusted_end:]\n",
    "        offset += len(str(new_substring)) - (end_index - start_index)\n",
    "    return result\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    for root, split in zip(ROOTS, SPLITS):\n",
    "        data = []\n",
    "        data_points = []\n",
    "        with open(root, 'r') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            str_data = data[i]['text']\n",
    "            re_data = data[i]['edits'][0][1]\n",
    "            modified_string = replace_multiple_substrings(str_data, [data[:3] for data in re_data])\n",
    "            \n",
    "            data_point = {\n",
    "                'original': str_data,\n",
    "                'corrected': modified_string\n",
    "            }\n",
    "            data_points.append(data_point)\n",
    "        \n",
    "        with open(os.path.join(save_dir, split+'.json'), 'w') as f:\n",
    "            json.dump(data_points, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2116 examples [00:00, 37634.62 examples/s]\n",
      "Generating train split: 159 examples [00:00, 15324.92 examples/s]\n",
      "Generating train split: 194 examples [00:00, 14464.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = load_dataset(\n",
    "                            'json', \n",
    "                            data_files='data/fce/final/train.json', \n",
    "                            split='train'\n",
    "                            )\n",
    "dataset_valid = load_dataset(\n",
    "                            'json', \n",
    "                            data_files='data/fce/final/valid.json', \n",
    "                            split='train'\n",
    "                            )\n",
    "dataset_test = load_dataset(\n",
    "                            'json', \n",
    "                            data_files='data/fce/final/test.json', \n",
    "                            split='train'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = 'results/grammar-error-correction'\n",
    "MODEL = 't5-small'\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 8\n",
    "EPOCHS = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(\n",
    "                        examples, \n",
    "                        tokenizer=tokenizer,\n",
    "                        MAX_LENGTH=MAX_LENGTH\n",
    "                        ):\n",
    "    inputs = [f\"rectify: {inc}\" for inc in examples['original']]\n",
    "    model_inputs = tokenizer(\n",
    "                            inputs, \n",
    "                            max_length=MAX_LENGTH, \n",
    "                            truncation=True,\n",
    "                            padding='max_length'\n",
    "                            )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "                        examples['corrected'], \n",
    "                        max_length=MAX_LENGTH, \n",
    "                        truncation=True,\n",
    "                        padding='max_length'\n",
    "                        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = dataset_train.map(\n",
    "                                    preprocess_function, \n",
    "                                    batched=True,\n",
    "                                    num_proc=8\n",
    "                                    )\n",
    "\n",
    "tokenized_valid = dataset_valid.map(\n",
    "                                    preprocess_function, \n",
    "                                    batched=True,\n",
    "                                    num_proc=8\n",
    "                                    )\n",
    "tokenized_test = dataset_test.map(\n",
    "                                preprocess_function, \n",
    "                                batched=True,\n",
    "                                num_proc=8\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60,506,624 total parameters.\n",
      "60,506,624 training parameters.\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isuru\\anaconda3\\envs\\uni\\lib\\site-packages\\accelerate\\accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "  5%|▌         | 500/9310 [21:02<4:43:24,  1.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.119, 'learning_rate': 5e-05, 'epoch': 3.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  5%|▌         | 500/9310 [21:23<4:43:24,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5771849155426025, 'eval_runtime': 21.0081, 'eval_samples_per_second': 7.569, 'eval_steps_per_second': 0.238, 'epoch': 3.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1000/9310 [38:26<4:26:12,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.548, 'learning_rate': 4.7162315550510784e-05, 'epoch': 7.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 11%|█         | 1000/9310 [38:46<4:26:12,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49688783288002014, 'eval_runtime': 20.4095, 'eval_samples_per_second': 7.79, 'eval_steps_per_second': 0.245, 'epoch': 7.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1500/9310 [55:49<4:13:17,  1.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4821, 'learning_rate': 4.432463110102157e-05, 'epoch': 11.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 16%|█▌        | 1500/9310 [56:09<4:13:17,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46675047278404236, 'eval_runtime': 20.3717, 'eval_samples_per_second': 7.805, 'eval_steps_per_second': 0.245, 'epoch': 11.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 2000/9310 [1:13:12<5:43:53,  2.82s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4444, 'learning_rate': 4.1486946651532346e-05, 'epoch': 15.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 21%|██▏       | 2000/9310 [1:13:32<5:43:53,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44291672110557556, 'eval_runtime': 20.1024, 'eval_samples_per_second': 7.909, 'eval_steps_per_second': 0.249, 'epoch': 15.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2500/9310 [1:30:24<3:39:15,  1.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4181, 'learning_rate': 3.8649262202043134e-05, 'epoch': 18.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 27%|██▋       | 2500/9310 [1:30:44<3:39:15,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4322322607040405, 'eval_runtime': 20.3395, 'eval_samples_per_second': 7.817, 'eval_steps_per_second': 0.246, 'epoch': 18.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3000/9310 [1:47:45<3:24:29,  1.94s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3974, 'learning_rate': 3.5811577752553915e-05, 'epoch': 22.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 32%|███▏      | 3000/9310 [1:48:06<3:24:29,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42515939474105835, 'eval_runtime': 20.453, 'eval_samples_per_second': 7.774, 'eval_steps_per_second': 0.244, 'epoch': 22.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3500/9310 [2:05:07<3:05:49,  1.92s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3805, 'learning_rate': 3.29738933030647e-05, 'epoch': 26.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 38%|███▊      | 3500/9310 [2:05:27<3:05:49,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41905856132507324, 'eval_runtime': 20.3572, 'eval_samples_per_second': 7.81, 'eval_steps_per_second': 0.246, 'epoch': 26.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 4000/9310 [2:22:29<3:03:42,  2.08s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.366, 'learning_rate': 3.013620885357548e-05, 'epoch': 30.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 43%|████▎     | 4000/9310 [2:22:49<3:03:42,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4152158200740814, 'eval_runtime': 20.1988, 'eval_samples_per_second': 7.872, 'eval_steps_per_second': 0.248, 'epoch': 30.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4500/9310 [2:39:38<2:37:06,  1.96s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3549, 'learning_rate': 2.7298524404086268e-05, 'epoch': 33.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 48%|████▊     | 4500/9310 [2:39:58<2:37:06,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4128681421279907, 'eval_runtime': 20.1272, 'eval_samples_per_second': 7.9, 'eval_steps_per_second': 0.248, 'epoch': 33.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 5000/9310 [2:56:59<2:19:12,  1.94s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3446, 'learning_rate': 2.446083995459705e-05, 'epoch': 37.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 54%|█████▎    | 5000/9310 [2:57:19<2:19:12,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4111403822898865, 'eval_runtime': 19.9924, 'eval_samples_per_second': 7.953, 'eval_steps_per_second': 0.25, 'epoch': 37.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 5500/9310 [3:14:22<2:03:04,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.338, 'learning_rate': 2.1623155505107834e-05, 'epoch': 41.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 59%|█████▉    | 5500/9310 [3:14:42<2:03:04,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4124952256679535, 'eval_runtime': 20.2133, 'eval_samples_per_second': 7.866, 'eval_steps_per_second': 0.247, 'epoch': 41.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 6000/9310 [3:31:44<1:50:56,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.33, 'learning_rate': 1.878547105561862e-05, 'epoch': 45.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 64%|██████▍   | 6000/9310 [3:32:05<1:50:56,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4117662310600281, 'eval_runtime': 20.5584, 'eval_samples_per_second': 7.734, 'eval_steps_per_second': 0.243, 'epoch': 45.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 6500/9310 [3:48:54<1:31:49,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3241, 'learning_rate': 1.59477866061294e-05, 'epoch': 48.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 70%|██████▉   | 6500/9310 [3:49:15<1:31:49,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4102106988430023, 'eval_runtime': 20.3813, 'eval_samples_per_second': 7.801, 'eval_steps_per_second': 0.245, 'epoch': 48.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7000/9310 [4:06:17<1:13:47,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3194, 'learning_rate': 1.3110102156640184e-05, 'epoch': 52.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 75%|███████▌  | 7000/9310 [4:06:38<1:13:47,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41092225909233093, 'eval_runtime': 20.7185, 'eval_samples_per_second': 7.674, 'eval_steps_per_second': 0.241, 'epoch': 52.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 7500/9310 [4:23:39<59:21,  1.97s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3155, 'learning_rate': 1.0272417707150965e-05, 'epoch': 56.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 81%|████████  | 7500/9310 [4:24:00<59:21,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4105730950832367, 'eval_runtime': 20.6932, 'eval_samples_per_second': 7.684, 'eval_steps_per_second': 0.242, 'epoch': 56.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 8000/9310 [4:41:04<43:11,  1.98s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3129, 'learning_rate': 7.434733257661748e-06, 'epoch': 60.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 86%|████████▌ | 8000/9310 [4:41:24<43:11,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4103817343711853, 'eval_runtime': 20.4779, 'eval_samples_per_second': 7.764, 'eval_steps_per_second': 0.244, 'epoch': 60.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 8500/9310 [4:58:15<26:07,  1.94s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3107, 'learning_rate': 4.5970488081725315e-06, 'epoch': 63.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 91%|█████████▏| 8500/9310 [4:58:36<26:07,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4108417332172394, 'eval_runtime': 20.5321, 'eval_samples_per_second': 7.744, 'eval_steps_per_second': 0.244, 'epoch': 63.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 9000/9310 [5:15:38<10:08,  1.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3086, 'learning_rate': 1.7593643586833145e-06, 'epoch': 67.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 97%|█████████▋| 9000/9310 [5:15:59<10:08,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41025298833847046, 'eval_runtime': 20.6367, 'eval_samples_per_second': 7.705, 'eval_steps_per_second': 0.242, 'epoch': 67.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9310/9310 [5:26:27<00:00,  1.95s/it]There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 9310/9310 [5:26:27<00:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 19587.4875, 'train_samples_per_second': 7.562, 'train_steps_per_second': 0.475, 'train_loss': 0.46214260425014475, 'epoch': 70.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "                                output_dir=OUT_DIR,          \n",
    "                                num_train_epochs=EPOCHS,\n",
    "                                per_device_train_batch_size=BATCH_SIZE,\n",
    "                                per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "                                warmup_steps=500,\n",
    "                                weight_decay=0.01,\n",
    "                                logging_dir=OUT_DIR,\n",
    "                                evaluation_strategy='steps',\n",
    "                                save_steps=500,\n",
    "                                eval_steps=500,\n",
    "                                load_best_model_at_end=True,\n",
    "                                save_total_limit=2,\n",
    "                                report_to='tensorboard',\n",
    "                                dataloader_num_workers=NUM_WORKERS\n",
    "                                )\n",
    "\n",
    "trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_train,\n",
    "                eval_dataset=tokenized_valid,\n",
    "                )\n",
    "\n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('models/grammar_error_detection')\n",
    "model.save_pretrained('models/grammar_error_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:16<00:00,  2.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.47226184606552124,\n",
       " 'eval_runtime': 29.7839,\n",
       " 'eval_samples_per_second': 6.514,\n",
       " 'eval_steps_per_second': 0.235,\n",
       " 'epoch': 70.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/grammar_error_detection'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_correction(text):\n",
    "    input_text = f\"rectify: {text}\"\n",
    "    inputs = tokenizer.encode(\n",
    "                            input_text,\n",
    "                            return_tensors='pt',\n",
    "                            max_length=256,\n",
    "                            padding='max_length',\n",
    "                            truncation=True\n",
    "                            )\n",
    "\n",
    "    corrected_ids = model.generate(\n",
    "                                inputs,\n",
    "                                max_length=384,\n",
    "                                num_beams=5,\n",
    "                                early_stopping=True\n",
    "                                )\n",
    "\n",
    "    corrected_sentence = tokenizer.decode(\n",
    "                                        corrected_ids[0],\n",
    "                                        skip_special_tokens=True\n",
    "                                        )\n",
    "    return corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG: He don't like to eat vegetables.\n",
      "CORRECT: He doesn't like to eat vegetables.\n",
      "ORIG: They was going to the store yesterday.\n",
      "CORRECT: They were going to the store yesterday.\n",
      "ORIG: She don't sings very well.\n",
      "CORRECT: She doesn't sing very well.\n",
      "ORIG: Between you and I, the decision not well received.\n",
      "CORRECT: Between you and I, the decision is not well received.\n",
      "ORIG: The book I borrowed from the library, it was really interesting.\n",
      "CORRECT: The book I borrowed from the library was really interesting.\n",
      "ORIG: Despite of the rain, they went for a picnic.\n",
      "CORRECT: Despite the rain, they went for a picnic.\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"He don't like to eat vegetables.\",\n",
    "    \"They was going to the store yesterday.\",\n",
    "    \"She don't sings very well.\",\n",
    "    \"Between you and I, the decision not well received.\",\n",
    "    \"The book I borrowed from the library, it was really interesting.\",\n",
    "    \"Despite of the rain, they went for a picnic.\"\n",
    "]\n",
    "for sentence in sentences:\n",
    "    corrected_sentence = do_correction(sentence)\n",
    "    print(f\"ORIG: {sentence}\\nCORRECT: {corrected_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elarning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
